gilbreth-c002.rcac.purdue.edu
Total tasks: 4
Total tasks per node: 4
CPUs per task: 2
CUDA visible devices: 0,1,2,3
Number of nodes: 1
The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) xalt/1.1.2
num_proc        : 2
Process rank: 0
World size: 4
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
num_proc        : 2
Process rank: 1
World size: 4
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
num_proc        : 2
Process rank: 2
World size: 4
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /home/ahmedb/.cache/huggingface/token
Login successful
Login to hub successful...!
Token is valid (permission: fineGrained).
Your token has been saved to /home/ahmedb/.cache/huggingface/token
Login successful
Login to hub successful...!
Token is valid (permission: fineGrained).
Your token has been saved to /home/ahmedb/.cache/huggingface/token
Login successful
Login to hub successful...!
num_proc        : 2
Process rank: 3
World size: 4
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /home/ahmedb/.cache/huggingface/token
Login successful
Login to hub successful...!
/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Starting training...!
Starting training...!
Starting training...!
Starting training...!
gilbreth-c002:101644:101644 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker,eth,lo
gilbreth-c002:101644:101644 [0] NCCL INFO Bootstrap : Using ib0:172.18.36.32<0>
gilbreth-c002:101644:101644 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
gilbreth-c002:101644:101644 [0] NCCL INFO cudaDriverVersion 12030
NCCL version 2.20.5+cuda11.0
gilbreth-c002:101645:101645 [1] NCCL INFO cudaDriverVersion 12030
gilbreth-c002:101645:101645 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker,eth,lo
gilbreth-c002:101646:101646 [2] NCCL INFO cudaDriverVersion 12030
gilbreth-c002:101645:101645 [1] NCCL INFO Bootstrap : Using ib0:172.18.36.32<0>
gilbreth-c002:101646:101646 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker,eth,lo
gilbreth-c002:101645:101645 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
gilbreth-c002:101646:101646 [2] NCCL INFO Bootstrap : Using ib0:172.18.36.32<0>
gilbreth-c002:101646:101646 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
gilbreth-c002:101647:101647 [3] NCCL INFO cudaDriverVersion 12030
gilbreth-c002:101647:101647 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker,eth,lo
gilbreth-c002:101647:101647 [3] NCCL INFO Bootstrap : Using ib0:172.18.36.32<0>
gilbreth-c002:101647:101647 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
gilbreth-c002:101645:102116 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker,eth,lo
gilbreth-c002:101644:102115 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker,eth,lo
gilbreth-c002:101646:102117 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker,eth,lo
gilbreth-c002:101644:102115 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:172.18.36.32<0>
gilbreth-c002:101644:102115 [0] NCCL INFO Using non-device net plugin version 0
gilbreth-c002:101644:102115 [0] NCCL INFO Using network IB
gilbreth-c002:101645:102116 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:172.18.36.32<0>
gilbreth-c002:101645:102116 [1] NCCL INFO Using non-device net plugin version 0
gilbreth-c002:101645:102116 [1] NCCL INFO Using network IB
gilbreth-c002:101646:102117 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:172.18.36.32<0>
gilbreth-c002:101646:102117 [2] NCCL INFO Using non-device net plugin version 0
gilbreth-c002:101646:102117 [2] NCCL INFO Using network IB
gilbreth-c002:101647:102121 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker,eth,lo
gilbreth-c002:101647:102121 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:172.18.36.32<0>
gilbreth-c002:101647:102121 [3] NCCL INFO Using non-device net plugin version 0
gilbreth-c002:101647:102121 [3] NCCL INFO Using network IB
gilbreth-c002:101644:102115 [0] NCCL INFO comm 0x559def7b8230 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 61000 commId 0xecf143863d1ab34a - Init START
gilbreth-c002:101645:102116 [1] NCCL INFO comm 0x56376eb66e30 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 62000 commId 0xecf143863d1ab34a - Init START
gilbreth-c002:101646:102117 [2] NCCL INFO comm 0x560fef94a4b0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 89000 commId 0xecf143863d1ab34a - Init START
gilbreth-c002:101647:102121 [3] NCCL INFO comm 0x5567fbc870b0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 8a000 commId 0xecf143863d1ab34a - Init START
gilbreth-c002:101644:102115 [0] NCCL INFO Setting affinity for GPU 0 to 03ff
gilbreth-c002:101646:102117 [2] NCCL INFO Setting affinity for GPU 2 to 0ffc00
gilbreth-c002:101645:102116 [1] NCCL INFO Setting affinity for GPU 1 to 03ff
gilbreth-c002:101647:102121 [3] NCCL INFO Setting affinity for GPU 3 to 0ffc00
gilbreth-c002:101644:102115 [0] NCCL INFO comm 0x559def7b8230 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 00/12 :    0   1   2   3
gilbreth-c002:101647:102121 [3] NCCL INFO comm 0x5567fbc870b0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 01/12 :    0   1   3   2
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 02/12 :    0   2   3   1
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 03/12 :    0   2   1   3
gilbreth-c002:101646:102117 [2] NCCL INFO comm 0x560fef94a4b0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 04/12 :    0   3   1   2
gilbreth-c002:101645:102116 [1] NCCL INFO comm 0x56376eb66e30 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 05/12 :    0   3   2   1
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 06/12 :    0   1   2   3
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 07/12 :    0   1   3   2
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 08/12 :    0   2   3   1
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 09/12 :    0   2   1   3
gilbreth-c002:101647:102121 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->1 [3] -1/-1/-1->3->1 [4] 2/-1/-1->3->0 [5] 2/-1/-1->3->0 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->1 [9] -1/-1/-1->3->1 [10] 2/-1/-1->3->0 [11] 2/-1/-1->3->0
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 10/12 :    0   3   1   2
gilbreth-c002:101647:102121 [3] NCCL INFO P2P Chunksize set to 524288
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 11/12 :    0   3   2   1
gilbreth-c002:101646:102117 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 1/-1/-1->2->0 [3] 1/-1/-1->2->0 [4] -1/-1/-1->2->3 [5] -1/-1/-1->2->3 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 1/-1/-1->2->0 [9] 1/-1/-1->2->0 [10] -1/-1/-1->2->3 [11] -1/-1/-1->2->3
gilbreth-c002:101644:102115 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 2/-1/-1->0->-1 [3] 2/-1/-1->0->-1 [4] 3/-1/-1->0->1 [5] 3/-1/-1->0->1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 2/-1/-1->0->-1 [9] 2/-1/-1->0->-1 [10] 3/-1/-1->0->1 [11] 3/-1/-1->0->1
gilbreth-c002:101644:102115 [0] NCCL INFO P2P Chunksize set to 524288
gilbreth-c002:101646:102117 [2] NCCL INFO P2P Chunksize set to 524288
gilbreth-c002:101645:102116 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 3/-1/-1->1->2 [3] 3/-1/-1->1->2 [4] 0/-1/-1->1->-1 [5] 0/-1/-1->1->-1 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 3/-1/-1->1->2 [9] 3/-1/-1->1->2 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1
gilbreth-c002:101645:102116 [1] NCCL INFO P2P Chunksize set to 524288
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 02/0 : 0[0] -> 2[2] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 03/0 : 0[0] -> 2[2] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 04/0 : 3[3] -> 1[1] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 01/0 : 1[1] -> 3[3] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 08/0 : 0[0] -> 2[2] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 08/0 : 3[3] -> 1[1] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 04/0 : 2[2] -> 0[0] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 03/0 : 1[1] -> 3[3] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 09/0 : 0[0] -> 2[2] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 10/0 : 3[3] -> 1[1] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 07/0 : 2[2] -> 0[0] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 07/0 : 1[1] -> 3[3] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 10/0 : 2[2] -> 0[0] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 09/0 : 1[1] -> 3[3] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 04/0 : 0[0] -> 3[3] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 05/0 : 0[0] -> 3[3] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 10/0 : 0[0] -> 3[3] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 11/0 : 0[0] -> 3[3] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Connected all rings
gilbreth-c002:101647:102121 [3] NCCL INFO Connected all rings
gilbreth-c002:101646:102117 [2] NCCL INFO Connected all rings
gilbreth-c002:101645:102116 [1] NCCL INFO Connected all rings
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 04/0 : 3[3] -> 0[0] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 05/0 : 3[3] -> 0[0] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 02/0 : 2[2] -> 0[0] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 03/0 : 2[2] -> 0[0] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 03/0 : 3[3] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 02/0 : 1[1] -> 3[3] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 08/0 : 2[2] -> 0[0] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 09/0 : 3[3] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 08/0 : 1[1] -> 3[3] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 09/0 : 2[2] -> 0[0] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC
gilbreth-c002:101647:102121 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC
gilbreth-c002:101646:102117 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/IPC
gilbreth-c002:101645:102116 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC
gilbreth-c002:101644:102115 [0] NCCL INFO Connected all trees
gilbreth-c002:101647:102121 [3] NCCL INFO Connected all trees
gilbreth-c002:101647:102121 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gilbreth-c002:101647:102121 [3] NCCL INFO 12 coll channels, 0 collnet channels, 0 nvls channels, 16 p2p channels, 4 p2p channels per peer
gilbreth-c002:101644:102115 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gilbreth-c002:101644:102115 [0] NCCL INFO 12 coll channels, 0 collnet channels, 0 nvls channels, 16 p2p channels, 4 p2p channels per peer
gilbreth-c002:101646:102117 [2] NCCL INFO Connected all trees
gilbreth-c002:101645:102116 [1] NCCL INFO Connected all trees
gilbreth-c002:101646:102117 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gilbreth-c002:101646:102117 [2] NCCL INFO 12 coll channels, 0 collnet channels, 0 nvls channels, 16 p2p channels, 4 p2p channels per peer
gilbreth-c002:101645:102116 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gilbreth-c002:101645:102116 [1] NCCL INFO 12 coll channels, 0 collnet channels, 0 nvls channels, 16 p2p channels, 4 p2p channels per peer
gilbreth-c002:101644:102115 [0] NCCL INFO comm 0x559def7b8230 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 61000 commId 0xecf143863d1ab34a - Init COMPLETE
gilbreth-c002:101646:102117 [2] NCCL INFO comm 0x560fef94a4b0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 89000 commId 0xecf143863d1ab34a - Init COMPLETE
gilbreth-c002:101647:102121 [3] NCCL INFO comm 0x5567fbc870b0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 8a000 commId 0xecf143863d1ab34a - Init COMPLETE
gilbreth-c002:101645:102116 [1] NCCL INFO comm 0x56376eb66e30 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 62000 commId 0xecf143863d1ab34a - Init COMPLETE
  0%|          | 0/4392 [00:00<?, ?it/s]  0%|          | 1/4392 [00:11<13:59:14, 11.47s/it]  0%|          | 2/4392 [00:15<8:53:36,  7.29s/it]   0%|          | 3/4392 [00:19<7:08:00,  5.85s/it][rank3]: Traceback (most recent call last):
[rank3]:   File "../scripts/pretrain_wav2vec2.py", line 133, in <module>
[rank3]:     train_wav2vec2(args)
[rank3]:   File "../scripts/pretrain_wav2vec2.py", line 100, in train_wav2vec2
[rank3]:     trainer.train()
[rank3]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 1923, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 2268, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs)
[rank3]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 3307, in training_step
[rank3]:     loss = self.compute_loss(model, inputs)
[rank3]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 3338, in compute_loss
[rank3]:     outputs = model(**inputs)
[rank3]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1589, in forward
[rank3]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank3]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1480, in _pre_forward
[rank3]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank3]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
[rank3]: making sure all `forward` function outputs participate in calculating loss. 
[rank3]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[rank3]: Parameter indices which did not receive grad for rank 3: 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146
[rank3]:  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[rank1]: Traceback (most recent call last):
[rank1]:   File "../scripts/pretrain_wav2vec2.py", line 133, in <module>
[rank1]:     train_wav2vec2(args)
[rank1]:   File "../scripts/pretrain_wav2vec2.py", line 100, in train_wav2vec2
[rank1]:     trainer.train()
[rank1]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 1923, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 2268, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs)
[rank1]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 3307, in training_step
[rank1]:     loss = self.compute_loss(model, inputs)
[rank1]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 3338, in compute_loss
[rank1]:     outputs = model(**inputs)
[rank1]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1589, in forward
[rank1]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank1]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1480, in _pre_forward
[rank1]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank1]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
[rank1]: making sure all `forward` function outputs participate in calculating loss. 
[rank1]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[rank1]: Parameter indices which did not receive grad for rank 1: 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146
[rank1]:  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[rank0]: Traceback (most recent call last):
[rank0]:   File "../scripts/pretrain_wav2vec2.py", line 133, in <module>
[rank0]:     train_wav2vec2(args)
[rank0]:   File "../scripts/pretrain_wav2vec2.py", line 100, in train_wav2vec2
[rank0]:     trainer.train()
[rank0]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 1923, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 2268, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs)
[rank0]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 3307, in training_step
[rank0]:     loss = self.compute_loss(model, inputs)
[rank0]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 3338, in compute_loss
[rank0]:     outputs = model(**inputs)
[rank0]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1589, in forward
[rank0]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank0]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1480, in _pre_forward
[rank0]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank0]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
[rank0]: making sure all `forward` function outputs participate in calculating loss. 
[rank0]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[rank0]: Parameter indices which did not receive grad for rank 0: 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146
[rank0]:  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
  0%|          | 3/4392 [00:22<9:09:21,  7.51s/it]
[rank2]: Traceback (most recent call last):
[rank2]:   File "../scripts/pretrain_wav2vec2.py", line 133, in <module>
[rank2]:     train_wav2vec2(args)
[rank2]:   File "../scripts/pretrain_wav2vec2.py", line 100, in train_wav2vec2
[rank2]:     trainer.train()
[rank2]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 1923, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 2268, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs)
[rank2]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 3307, in training_step
[rank2]:     loss = self.compute_loss(model, inputs)
[rank2]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/transformers/trainer.py", line 3338, in compute_loss
[rank2]:     outputs = model(**inputs)
[rank2]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1589, in forward
[rank2]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank2]:   File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1480, in _pre_forward
[rank2]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank2]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
[rank2]: making sure all `forward` function outputs participate in calculating loss. 
[rank2]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[rank2]: Parameter indices which did not receive grad for rank 2: 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146
[rank2]:  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
W0721 13:15:51.770859 47080320134848 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 101646 closing signal SIGTERM
E0721 13:15:52.086388 47080320134848 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 101644) of binary: /home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/bin/python
Traceback (most recent call last):
  File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
../scripts/pretrain_wav2vec2.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-07-21_13:15:51
  host      : gilbreth-c002.rcac.purdue.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 101645)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-07-21_13:15:51
  host      : gilbreth-c002.rcac.purdue.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 101647)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-21_13:15:51
  host      : gilbreth-c002.rcac.purdue.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 101644)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
